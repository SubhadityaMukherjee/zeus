{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "charitable-sapphire",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-copying",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")\n",
    "os.environ[\"TORCH_HOME\"] = \"/media/hdd/Datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn import metrics, model_selection, preprocessing\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zeus\n",
    "from zeus.callbacks import (EarlyStopping, GradientClipping, PlotLoss,\n",
    "                            TensorBoardLogger)\n",
    "from zeus.datasets import ImageDataset\n",
    "from zeus.metrics import LabelSmoothingCrossEntropy\n",
    "from zeus.utils.model_helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-insured",
   "metadata": {},
   "source": [
    "# Defining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-movie",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = \"/media/hdd/Datasets/blindness/\"\n",
    "MODEL_PATH = \"./models/\"\n",
    "MODEL_NAME = os.path.basename(\"blindness.pt\")\n",
    "TRAIN_BATCH_SIZE = 140\n",
    "VALID_BATCH_SIZE = 140\n",
    "IMAGE_SIZE = 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(zeus.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.effnet = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(1280, num_classes)\n",
    "\n",
    "    def monitor_metrics(self, outputs, targets):\n",
    "        outputs = torch.argmax(outputs, dim=1).cpu().detach().numpy()\n",
    "        targets = targets.cpu().detach().numpy()\n",
    "        accuracy = metrics.accuracy_score(targets, outputs)\n",
    "        return {\"accuracy\": accuracy}\n",
    "\n",
    "    def fetch_optimizer(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
    "        return opt\n",
    "\n",
    "    def forward(self, image, targets=None):\n",
    "        batch_size, _, _, _ = image.shape\n",
    "\n",
    "        x = self.effnet.extract_features(image)\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(batch_size, -1)\n",
    "        outputs = self.out(self.dropout(x))\n",
    "\n",
    "        if targets is not None:\n",
    "            #  loss = nn.CrossEntropyLoss()(outputs, targets)\n",
    "            loss = LabelSmoothingCrossEntropy()(outputs, targets)\n",
    "            metrics = self.monitor_metrics(outputs, targets)\n",
    "            return outputs, loss, metrics\n",
    "        return outputs, 0, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-venture",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        albumentations.Transpose(p=0.5),\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.VerticalFlip(p=0.5),\n",
    "        albumentations.ShiftScaleRotate(p=0.5),\n",
    "        albumentations.HueSaturationValue(\n",
    "            hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5\n",
    "        ),\n",
    "        albumentations.RandomBrightnessContrast(\n",
    "            brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5\n",
    "        ),\n",
    "        albumentations.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "            max_pixel_value=255.0,\n",
    "            p=1.0,\n",
    "        ),\n",
    "    ],\n",
    "    p=1.0,\n",
    ")\n",
    "\n",
    "valid_aug = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        albumentations.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "            max_pixel_value=255.0,\n",
    "            p=1.0,\n",
    "        ),\n",
    "    ],\n",
    "    p=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-writer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_image_paths = glob.glob(\n",
    "#     os.path.join(\n",
    "#         INPUT_PATH, f\"jpeg-{IMAGE_SIZE}x{IMAGE_SIZE}\", \"train\", \"**\", \"*.jpeg\"\n",
    "#     ),\n",
    "#     recursive=True,\n",
    "# )\n",
    "\n",
    "# valid_image_paths = glob.glob(\n",
    "#     os.path.join(\n",
    "#         INPUT_PATH, f\"jpeg-{IMAGE_SIZE}x{IMAGE_SIZE}\", \"val\", \"**\", \"*.jpeg\"\n",
    "#     ),\n",
    "#     recursive=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_targets = [x.split(\"/\")[-2] for x in train_image_paths]\n",
    "# valid_targets = [x.split(\"/\")[-2] for x in valid_image_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-vermont",
   "metadata": {},
   "source": [
    "## Data pre process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-brooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(INPUT_PATH + \"trainLabels.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"image\"] = INPUT_PATH + \"trainImages/\" + df[\"image\"] + \".jpg\"\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-management",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBSET REMOVE LATER\n",
    "df = df.head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, valid_images = train_test_split(df, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_paths, valid_image_paths = (\n",
    "    train_images[\"image\"].values,\n",
    "    valid_images[\"image\"].values,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets, valid_targets = (\n",
    "    train_images[\"level\"].values,\n",
    "    valid_images[\"level\"].values,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-nowhere",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "train_targets = lbl_enc.fit_transform(train_targets)\n",
    "valid_targets = lbl_enc.transform(valid_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(\n",
    "    image_paths=train_image_paths,\n",
    "    targets=train_targets,\n",
    "    augmentations=train_aug,\n",
    ")\n",
    "\n",
    "valid_dataset = ImageDataset(\n",
    "    image_paths=valid_image_paths,\n",
    "    targets=valid_targets,\n",
    "    augmentations=valid_aug,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-oregon",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_classes=len(lbl_enc.classes_))\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor=\"valid_loss\",\n",
    "    model_path=os.path.join(MODEL_PATH, MODEL_NAME + \".bin\"),\n",
    "    patience=3,\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "tb = TensorBoardLogger()\n",
    "grc = GradientClipping(5)\n",
    "pl = PlotLoss(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(model, showtable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    valid_dataset=valid_dataset,\n",
    "    train_bs=TRAIN_BATCH_SIZE,\n",
    "    valid_bs=VALID_BATCH_SIZE,\n",
    "    device=\"cuda\",\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[grc, pl, tb],\n",
    "    #     callbacks=[es, tb],\n",
    "    fp16=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
