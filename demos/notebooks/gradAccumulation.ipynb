{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160a5c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "os.environ[\"TORCH_HOME\"] = \"/media/hdd/Datasets/\"\n",
    "\n",
    "import glob\n",
    "\n",
    "import albumentations\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn import metrics, model_selection, preprocessing\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import zeus\n",
    "from zeus.callbacks import (EarlyStopping, GradientClipping, PlotLoss,\n",
    "                            PruningCallback, TensorBoardLogger)\n",
    "from zeus.datasets import ImageDataset\n",
    "from zeus.metrics import LabelSmoothingCrossEntropy\n",
    "from zeus.utils.model_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128df378",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Defining\n",
    "\n",
    "## Params\n",
    "\n",
    "INPUT_PATH = \"/media/hdd/Datasets/blindness/\"\n",
    "MODEL_PATH = \"./models/\"\n",
    "MODEL_NAME = os.path.basename(\"blindness.pt\")\n",
    "TRAIN_BATCH_SIZE = 64  # lower batch sizes\n",
    "VALID_BATCH_SIZE = 64\n",
    "IMAGE_SIZE = 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b4b926",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(zeus.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.effnet = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(1280, num_classes)\n",
    "\n",
    "    def monitor_metrics(self, outputs, targets):\n",
    "        outputs = torch.argmax(outputs, dim=1).cpu().detach().numpy()\n",
    "        targets = targets.cpu().detach().numpy()\n",
    "        accuracy = metrics.accuracy_score(targets, outputs)\n",
    "        return {\"accuracy\": accuracy}\n",
    "\n",
    "    def fetch_optimizer(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
    "        return opt\n",
    "\n",
    "    def forward(self, image, targets=None):\n",
    "        batch_size, _, _, _ = image.shape\n",
    "\n",
    "        x = self.effnet.extract_features(image)\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(batch_size, -1)\n",
    "        outputs = self.out(self.dropout(x))\n",
    "\n",
    "        if targets is not None:\n",
    "            #  loss = nn.CrossEntropyLoss()(outputs, targets)\n",
    "            loss = LabelSmoothingCrossEntropy()(outputs, targets)\n",
    "            metrics = self.monitor_metrics(outputs, targets)\n",
    "            return outputs, loss, metrics\n",
    "        return outputs, 0, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5756b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        albumentations.Transpose(p=0.5),\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.VerticalFlip(p=0.5),\n",
    "        albumentations.ShiftScaleRotate(p=0.5),\n",
    "        albumentations.HueSaturationValue(\n",
    "            hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5\n",
    "        ),\n",
    "        albumentations.RandomBrightnessContrast(\n",
    "            brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5\n",
    "        ),\n",
    "        albumentations.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "            max_pixel_value=255.0,\n",
    "            p=1.0,\n",
    "        ),\n",
    "    ],\n",
    "    p=1.0,\n",
    ")\n",
    "\n",
    "valid_aug = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        albumentations.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "            max_pixel_value=255.0,\n",
    "            p=1.0,\n",
    "        ),\n",
    "    ],\n",
    "    p=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184f9294",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#  Data pre process\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH + \"trainLabels.csv\")\n",
    "df.head(3)\n",
    "\n",
    "df[\"image\"] = INPUT_PATH + \"trainImages/\" + df[\"image\"] + \".jpg\"\n",
    "df.head(3)\n",
    "\n",
    "# SUBSET REMOVE LATER\n",
    "df = df.head(5000)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_images, valid_images = train_test_split(df, test_size=0.33)\n",
    "\n",
    "train_image_paths, valid_image_paths = (\n",
    "    train_images[\"image\"].values,\n",
    "    valid_images[\"image\"].values,\n",
    ")\n",
    "\n",
    "train_targets, valid_targets = (\n",
    "    train_images[\"level\"].values,\n",
    "    valid_images[\"level\"].values,\n",
    ")\n",
    "\n",
    "\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "train_targets = lbl_enc.fit_transform(train_targets)\n",
    "valid_targets = lbl_enc.transform(valid_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79179f3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "train_dataset = ImageDataset(\n",
    "    image_paths=train_image_paths,\n",
    "    targets=train_targets,\n",
    "    augmentations=train_aug,\n",
    ")\n",
    "\n",
    "valid_dataset = ImageDataset(\n",
    "    image_paths=valid_image_paths,\n",
    "    targets=valid_targets,\n",
    "    augmentations=valid_aug,\n",
    ")\n",
    "# -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9af2e3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#  Callbacks\n",
    "model = Model(num_classes=len(lbl_enc.classes_))\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor=\"valid_loss\",\n",
    "    model_path=os.path.join(MODEL_PATH, MODEL_NAME + \".bin\"),\n",
    "    patience=3,\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "tb = TensorBoardLogger()\n",
    "grc = GradientClipping(5)\n",
    "pr = PruningCallback()\n",
    "\n",
    "count_parameters(model, showtable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bce456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "pl = PlotLoss(EPOCHS)\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    valid_dataset=valid_dataset,\n",
    "    train_bs=TRAIN_BATCH_SIZE,\n",
    "    valid_bs=VALID_BATCH_SIZE,\n",
    "    device=\"cuda\",\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[grc, pl, tb, pr],\n",
    "    fp16=True,\n",
    "    pin_mem=True,\n",
    "    accumulation_steps=3,  # just add this\n",
    ")\n",
    "# -"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
