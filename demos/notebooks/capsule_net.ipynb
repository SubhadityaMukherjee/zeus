{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-supplement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Imports\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "os.environ[\"TORCH_HOME\"] = \"/media/hdd/Datasets/\"\n",
    "\n",
    "import glob\n",
    "\n",
    "import albumentations\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from PIL import Image\n",
    "from sklearn import metrics, model_selection, preprocessing\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import zeus\n",
    "from zeus.callbacks import (EarlyStopping, GradientClipping, PlotLoss,\n",
    "                            TensorBoardLogger)\n",
    "from zeus.datasets import ImageDataset\n",
    "from zeus.metrics import LabelSmoothingCrossEntropy\n",
    "from zeus.utils.model_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining\n",
    "\n",
    "# ## Params\n",
    "\n",
    "# INPUT_PATH = \"/media/hdd/Datasets/blindness/\"\n",
    "MODEL_PATH = \"./models/\"\n",
    "MODEL_NAME = os.path.basename(\"blindness.pt\")\n",
    "TRAIN_BATCH_SIZE = 140\n",
    "VALID_BATCH_SIZE = 140\n",
    "IMAGE_SIZE = 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-mustang",
   "metadata": {
    "title": "CapsNet https://github.com/jindongwang/Pytorch-CapsuleNet/blob/master/capsnet.py"
   },
   "outputs": [],
   "source": [
    "USE_CUDA = True if torch.cuda.is_available() else False\n",
    "\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=256, kernel_size=9):\n",
    "        super(ConvLayer, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.conv(x))\n",
    "\n",
    "\n",
    "class PrimaryCaps(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_capsules=8,\n",
    "        in_channels=256,\n",
    "        out_channels=32,\n",
    "        kernel_size=9,\n",
    "        num_routes=32 * 6 * 6,\n",
    "    ):\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "        self.num_routes = num_routes\n",
    "        self.capsules = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=2,\n",
    "                    padding=0,\n",
    "                )\n",
    "                for _ in range(num_capsules)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = [capsule(x) for capsule in self.capsules]\n",
    "        u = torch.stack(u, dim=1)\n",
    "        u = u.view(x.size(0), self.num_routes, -1)\n",
    "        return self.squash(u)\n",
    "\n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = (\n",
    "            squared_norm\n",
    "            * input_tensor\n",
    "            / ((1.0 + squared_norm) * torch.sqrt(squared_norm))\n",
    "        )\n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "class DigitCaps(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_capsules=10, num_routes=32 * 6 * 6, in_channels=8, out_channels=16\n",
    "    ):\n",
    "        super(DigitCaps, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_routes = num_routes\n",
    "        self.num_capsules = num_capsules\n",
    "\n",
    "        self.W = nn.Parameter(\n",
    "            torch.randn(1, num_routes, num_capsules, out_channels, in_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.stack([x] * self.num_capsules, dim=2).unsqueeze(4)\n",
    "\n",
    "        W = torch.cat([self.W] * batch_size, dim=0)\n",
    "        u_hat = torch.matmul(W, x)\n",
    "\n",
    "        b_ij = Variable(torch.zeros(1, self.num_routes, self.num_capsules, 1))\n",
    "        if USE_CUDA:\n",
    "            b_ij = b_ij.cuda()\n",
    "\n",
    "        num_iterations = 3\n",
    "        for iteration in range(num_iterations):\n",
    "            c_ij = F.softmax(b_ij, dim=1)\n",
    "            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(4)\n",
    "\n",
    "            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n",
    "            v_j = self.squash(s_j)\n",
    "\n",
    "            if iteration < num_iterations - 1:\n",
    "                a_ij = torch.matmul(\n",
    "                    u_hat.transpose(3, 4), torch.cat([v_j] * self.num_routes, dim=1)\n",
    "                )\n",
    "                b_ij = b_ij + a_ij.squeeze(4).mean(dim=0, keepdim=True)\n",
    "\n",
    "        return v_j.squeeze(1)\n",
    "\n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = (\n",
    "            squared_norm\n",
    "            * input_tensor\n",
    "            / ((1.0 + squared_norm) * torch.sqrt(squared_norm))\n",
    "        )\n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_width=28, input_height=28, input_channel=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_width = input_width\n",
    "        self.input_height = input_height\n",
    "        self.input_channel = input_channel\n",
    "        self.reconstraction_layers = nn.Sequential(\n",
    "            nn.Linear(16 * 10, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, self.input_height * self.input_width * self.input_channel),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, data):\n",
    "        classes = torch.sqrt((x ** 2).sum(2))\n",
    "        classes = F.softmax(classes, dim=0)\n",
    "\n",
    "        _, max_length_indices = classes.max(dim=1)\n",
    "        masked = Variable(torch.sparse.torch.eye(10))\n",
    "        if USE_CUDA:\n",
    "            masked = masked.cuda()\n",
    "        masked = masked.index_select(\n",
    "            dim=0, index=Variable(max_length_indices.squeeze(1).data)\n",
    "        )\n",
    "        t = (x * masked[:, :, None, None]).view(x.size(0), -1)\n",
    "        reconstructions = self.reconstraction_layers(t)\n",
    "        reconstructions = reconstructions.view(\n",
    "            -1, self.input_channel, self.input_width, self.input_height\n",
    "        )\n",
    "        return reconstructions, masked\n",
    "\n",
    "\n",
    "class CapsNet(nn.Module):\n",
    "    def __init__(self, config=None):\n",
    "        super(CapsNet, self).__init__()\n",
    "        if config:\n",
    "            self.conv_layer = ConvLayer(\n",
    "                config.cnn_in_channels, config.cnn_out_channels, config.cnn_kernel_size\n",
    "            )\n",
    "            self.primary_capsules = PrimaryCaps(\n",
    "                config.pc_num_capsules,\n",
    "                config.pc_in_channels,\n",
    "                config.pc_out_channels,\n",
    "                config.pc_kernel_size,\n",
    "                config.pc_num_routes,\n",
    "            )\n",
    "            self.digit_capsules = DigitCaps(\n",
    "                config.dc_num_capsules,\n",
    "                config.dc_num_routes,\n",
    "                config.dc_in_channels,\n",
    "                config.dc_out_channels,\n",
    "            )\n",
    "            self.decoder = Decoder(\n",
    "                config.input_width, config.input_height, config.cnn_in_channels\n",
    "            )\n",
    "        else:\n",
    "            self.conv_layer = ConvLayer()\n",
    "            self.primary_capsules = PrimaryCaps()\n",
    "            self.digit_capsules = DigitCaps()\n",
    "            self.decoder = Decoder()\n",
    "\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, data):\n",
    "        output = self.digit_capsules(self.primary_capsules(self.conv_layer(data)))\n",
    "        reconstructions, masked = self.decoder(output, data)\n",
    "        return output, reconstructions, masked\n",
    "\n",
    "    def loss(self, data, x, target, reconstructions):\n",
    "        return self.margin_loss(x, target) + self.reconstruction_loss(\n",
    "            data, reconstructions\n",
    "        )\n",
    "\n",
    "    def margin_loss(self, x, labels, size_average=True):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        v_c = torch.sqrt((x ** 2).sum(dim=2, keepdim=True))\n",
    "\n",
    "        left = F.relu(0.9 - v_c).view(batch_size, -1)\n",
    "        right = F.relu(v_c - 0.1).view(batch_size, -1)\n",
    "\n",
    "        loss = labels * left + 0.5 * (1.0 - labels) * right\n",
    "        loss = loss.sum(dim=1).mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def reconstruction_loss(self, data, reconstructions):\n",
    "        loss = self.mse_loss(\n",
    "            reconstructions.view(reconstructions.size(0), -1),\n",
    "            data.view(reconstructions.size(0), -1),\n",
    "        )\n",
    "        return loss * 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(zeus.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv_layer = ConvLayer()\n",
    "        self.primary_capsules = PrimaryCaps()\n",
    "        self.digit_capsules = DigitCaps()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def monitor_metrics(self, accuracy):\n",
    "        return {\"loss\": accuracy, \"epoch\": self.current_epoch}\n",
    "\n",
    "    def fetch_optimizer(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
    "        return opt\n",
    "\n",
    "    def margin_loss(self, x, labels, size_average=True):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        v_c = torch.sqrt((x ** 2).sum(dim=2, keepdim=True))\n",
    "\n",
    "        left = F.relu(0.9 - v_c).view(batch_size, -1)\n",
    "        right = F.relu(v_c - 0.1).view(batch_size, -1)\n",
    "\n",
    "        loss = labels * left + 0.5 * (1.0 - labels) * right\n",
    "        loss = loss.sum(dim=1).mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def reconstruction_loss(self, data, reconstructions):\n",
    "        loss = self.mse_loss(\n",
    "            reconstructions.view(reconstructions.size(0), -1),\n",
    "            data.view(reconstructions.size(0), -1),\n",
    "        )\n",
    "        return loss * 0.0005\n",
    "\n",
    "    def forward(self, image, targets=None):\n",
    "        batch_size, _, _, _ = image.shape\n",
    "        outputs = self.digit_capsules(self.primary_capsules(self.conv_layer(image)))\n",
    "        reconstructions, masked = self.decoder(outputs, image)\n",
    "\n",
    "        if targets is not None:\n",
    "            loss = self.margin_loss(outputs, targets) + self.reconstruction_loss(\n",
    "                image, reconstructions\n",
    "            )\n",
    "\n",
    "            metrics = self.monitor_metrics(loss)\n",
    "            return outputs, loss, metrics\n",
    "        return outputs, 0, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-knife",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "# +\n",
    "train_aug = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        albumentations.Transpose(p=0.5),\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.VerticalFlip(p=0.5),\n",
    "        albumentations.ShiftScaleRotate(p=0.5),\n",
    "        albumentations.HueSaturationValue(\n",
    "            hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5\n",
    "        ),\n",
    "        albumentations.RandomBrightnessContrast(\n",
    "            brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5\n",
    "        ),\n",
    "        albumentations.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "            max_pixel_value=255.0,\n",
    "            p=1.0,\n",
    "        ),\n",
    "    ],\n",
    "    p=1.0,\n",
    ")\n",
    "\n",
    "valid_aug = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        albumentations.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "            max_pixel_value=255.0,\n",
    "            p=1.0,\n",
    "        ),\n",
    "    ],\n",
    "    p=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-denial",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "# ## Training\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class Transforms:\n",
    "    # Compatibility with image folder https://gitmemory.com/issue/albumentations-team/albumentations/879/824771225\n",
    "    def __init__(self, transforms: albumentations.Compose):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, *args, **kwargs):\n",
    "        return self.transforms(image=np.array(img))[\"image\"]\n",
    "\n",
    "\n",
    "class cifar10(torchvision.datasets.CIFAR10):\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return {\"image\": img, \"targets\": target}\n",
    "\n",
    "\n",
    "train_dataset = cifar10(\n",
    "    \"/media/hdd/Datasets/\", transform=Transforms(train_aug), train=True\n",
    ")\n",
    "valid_dataset = cifar10(\n",
    "    \"/media/hdd/Datasets/\", transform=Transforms(valid_aug), train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-magazine",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# ## Callbacks\n",
    "\n",
    "model = Model(10)\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor=\"valid_loss\",\n",
    "    model_path=os.path.join(MODEL_PATH, MODEL_NAME + \".bin\"),\n",
    "    patience=3,\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "tb = TensorBoardLogger()\n",
    "grc = GradientClipping(5)\n",
    "pl = PlotLoss(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-lying",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "count_parameters(model, showtable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# +\n",
    "EPOCHS = 2\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    valid_dataset=valid_dataset,\n",
    "    train_bs=TRAIN_BATCH_SIZE,\n",
    "    valid_bs=VALID_BATCH_SIZE,\n",
    "    device=\"cuda\",\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[grc, pl, tb],\n",
    "    #     callbacks=[es, tb],\n",
    "    fp16=True,\n",
    ")\n",
    "# -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-processing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-fitness",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-skill",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-crash",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-township",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-portland",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-judge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-canvas",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
