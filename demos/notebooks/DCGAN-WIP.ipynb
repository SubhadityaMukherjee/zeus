{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-validity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "os.environ[\"TORCH_HOME\"] = \"/media/hdd/Datasets/\"\n",
    "\n",
    "import glob\n",
    "\n",
    "import albumentations\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn import metrics, model_selection, preprocessing\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import zeus\n",
    "from zeus.callbacks import (EarlyStopping, GradientClipping, PlotLoss,\n",
    "                            TensorBoardLogger)\n",
    "from zeus.datasets import ImageDataset\n",
    "from zeus.metrics import LabelSmoothingCrossEntropy\n",
    "from zeus.utils.model_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-documentation",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Defining\n",
    "\n",
    "# Params\n",
    "\n",
    "INPUT_PATH = \"/media/hdd/Datasets/animefacedataset/\"\n",
    "MODEL_PATH = \"./models/\"\n",
    "MODEL_NAME = os.path.basename(\"blindness.pt\")\n",
    "TRAIN_BATCH_SIZE = 140\n",
    "VALID_BATCH_SIZE = 140\n",
    "IMAGE_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-weather",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Models"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            # input: N x channels_img x 64 x 64\n",
    "            nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
    "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            # nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Input: N x channels_noise x 1 x 1\n",
    "            self._block(channels_noise, features_g * 16, 4, 1, 0),  # img: 4x4\n",
    "            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
    "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
    "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
    "            nn.ConvTranspose2d(\n",
    "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            # Output: N x channels_img x 64 x 64\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            # nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def initialize_weights(model):\n",
    "    # Initializes weights according to the DCGAN paper\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-roller",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Model(zeus.Model):\n",
    "    def __init__(self, nc, noise_dim, features_d, features_g):\n",
    "        super().__init__()\n",
    "        self.nc = nc\n",
    "        self.noise_dim = noise_dim\n",
    "        self.features_d = features_d\n",
    "        self.features_g = features_g\n",
    "        self.gen = Generator(self.noise_dim, self.nc, self.features_g)\n",
    "        self.disc = Discriminator(self.nc, self.features_d)\n",
    "        initialize_weights(self.gen)\n",
    "        initialize_weights(self.disc)\n",
    "\n",
    "    def monitor_metrics(self, lossd, lossg):\n",
    "        return {\"loss_D\": lossd, \"loss_G\": lossg}\n",
    "\n",
    "    def fetch_optimizer(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
    "        return opt\n",
    "\n",
    "    def forward(self, image, targets=None):\n",
    "        batch_size, _, _, _ = image.shape\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        image = image.to(self.device)\n",
    "        noise = torch.randn(batch_size, self.noise_dim, 1, 1).to(self.device)\n",
    "        fake = self.gen(noise)\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        disc_real = self.disc(image).reshape(-1)\n",
    "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = self.disc(fake.detach()).reshape(-1)\n",
    "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "        self.disc.zero_grad()\n",
    "        loss_disc.backward(retain_graph=True)\n",
    "        # opt_disc.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        output = self.disc(fake).reshape(-1)\n",
    "        loss_gen = criterion(output, torch.ones_like(output))\n",
    "        self.gen.zero_grad()\n",
    "        loss_gen.backward(retain_graph=True)\n",
    "        # opt_gen.step()\n",
    "        metrics = self.monitor_metrics(loss_disc, loss_gen)\n",
    "        return output, loss_gen, metrics\n",
    "\n",
    "        # if targets is not None:\n",
    "        #     #  loss = nn.CrossEntropyLoss()(outputs, targets)\n",
    "        #     loss = LabelSmoothingCrossEntropy()(outputs, targets)\n",
    "        #     metrics = self.monitor_metrics(outputs, targets)\n",
    "        #     return outputs, loss, metrics\n",
    "        # return outputs, 0, {}\n",
    "\n",
    "\n",
    "# +\n",
    "train_aug = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        albumentations.Transpose(p=0.5),\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.VerticalFlip(p=0.5),\n",
    "        albumentations.ShiftScaleRotate(p=0.5),\n",
    "        albumentations.HueSaturationValue(\n",
    "            hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5\n",
    "        ),\n",
    "        albumentations.RandomBrightnessContrast(\n",
    "            brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5\n",
    "        ),\n",
    "        albumentations.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "            max_pixel_value=255.0,\n",
    "            p=1.0,\n",
    "        ),\n",
    "    ],\n",
    "    p=1.0,\n",
    ")\n",
    "\n",
    "valid_aug = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        albumentations.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "            max_pixel_value=255.0,\n",
    "            p=1.0,\n",
    "        ),\n",
    "    ],\n",
    "    p=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-constant",
   "metadata": {
    "title": "Data read"
   },
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = [x for x in Path.iterdir(Path(INPUT_PATH) / \"anime\")]\n",
    "\n",
    "df = df[:1000]  # subset_test\n",
    "\n",
    "train_images, valid_images = train_test_split(df, test_size=0.33)\n",
    "train_labels, valid_labels = [1 for _ in range(len(train_images))], [\n",
    "    1 for _ in range(len(valid_images))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-battle",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "#  Training\n",
    "train_dataset = ImageDataset(\n",
    "    image_paths=train_images,\n",
    "    targets=train_labels,\n",
    "    augmentations=train_aug,\n",
    ")\n",
    "\n",
    "valid_dataset = ImageDataset(\n",
    "    image_paths=valid_images,\n",
    "    targets=valid_labels,\n",
    "    augmentations=valid_aug,\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "nc = 3\n",
    "noise_dim = 100\n",
    "features_d = 64\n",
    "features_g = 64\n",
    "model = Model(nc, noise_dim=noise_dim, features_d=features_d, features_g=features_g)\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor=\"valid_loss\",\n",
    "    model_path=os.path.join(MODEL_PATH, MODEL_NAME + \".bin\"),\n",
    "    patience=3,\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "tb = TensorBoardLogger()\n",
    "grc = GradientClipping(5)\n",
    "pl = PlotLoss(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(model, showtable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-circle",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    valid_dataset=valid_dataset,\n",
    "    train_bs=TRAIN_BATCH_SIZE,\n",
    "    valid_bs=VALID_BATCH_SIZE,\n",
    "    device=\"cuda\",\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[grc, pl, tb],\n",
    "    #     callbacks=[es, tb],\n",
    "    fp16=False,\n",
    ")\n",
    "# -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-journalist",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-painting",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
