{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-clone",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "os.environ[\"TORCH_HOME\"] = \"/media/hdd/Datasets/\"\n",
    "\n",
    "import glob\n",
    "import tarfile\n",
    "\n",
    "import albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-magic",
   "metadata": {
    "title": "Audio imports"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn import metrics, model_selection, preprocessing\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "import zeus\n",
    "from zeus.callbacks import (EarlyStopping, GradientClipping, PlotLoss,\n",
    "                            TensorBoardLogger)\n",
    "from zeus.datasets import ImageDataset\n",
    "from zeus.metrics import LabelSmoothingCrossEntropy, accuracy\n",
    "from zeus.utils.model_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-advantage",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Defining\n",
    "\n",
    "## Params\n",
    "\n",
    "INPUT_PATH = \"/media/hdd/Datasets/UrbanSound8K\"\n",
    "MODEL_PATH = \"./models/\"\n",
    "MODEL_NAME = os.path.basename(\"urban.pt\")\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "VALID_BATCH_SIZE = 128\n",
    "IMAGE_SIZE = 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-offering",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(zeus.Model):\n",
    "    def __init__(self, num_classes, input_size=40):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def monitor_metrics(self, acc):\n",
    "        # return {\"accuracy\": acc,\"epoch\": self.current_epoch}\n",
    "        return {\"epoch\": self.current_epoch}\n",
    "\n",
    "    def fetch_optimizer(self):\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "        return opt\n",
    "\n",
    "    def forward(self, image, targets=None):\n",
    "        # batch_size, _, _ = image.shape\n",
    "\n",
    "        outputs = self.network(image)\n",
    "\n",
    "        if targets is not None:\n",
    "            #  loss = nn.CrossEntropyLoss()(outputs, targets)\n",
    "            # loss = LabelSmoothingCrossEntropy()(outputs, targets)\n",
    "            outputs = torch.einsum(\"bxy->byx\", outputs)\n",
    "            loss = F.cross_entropy(outputs, targets.long())\n",
    "            acc = accuracy(outputs, targets.long())\n",
    "            metrics = self.monitor_metrics(acc)\n",
    "            return outputs, loss, metrics\n",
    "        return outputs, 0, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-murray",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "train_aug = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        albumentations.Transpose(p=0.5),\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.VerticalFlip(p=0.5),\n",
    "        albumentations.ShiftScaleRotate(p=0.5),\n",
    "        albumentations.HueSaturationValue(\n",
    "            hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5\n",
    "        ),\n",
    "        albumentations.RandomBrightnessContrast(\n",
    "            brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5\n",
    "        ),\n",
    "        albumentations.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "            max_pixel_value=255.0,\n",
    "            p=1.0,\n",
    "        ),\n",
    "    ],\n",
    "    p=1.0,\n",
    ")\n",
    "\n",
    "valid_aug = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        albumentations.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "            max_pixel_value=255.0,\n",
    "            p=1.0,\n",
    "        ),\n",
    "    ],\n",
    "    p=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-twelve",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(path):\n",
    "    audio, sr = librosa.load(path)\n",
    "    mfccs = librosa.feature.mfcc(audio, sr, n_mfcc=40)\n",
    "    return torch.tensor(np.mean(mfccs.T, axis=0))\n",
    "\n",
    "\n",
    "#  Data pre process\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH + \"/metadata/UrbanSound8K.csv\")\n",
    "df.head(3)\n",
    "print(df.shape)\n",
    "\n",
    "# SUBSET REMOVE LATER\n",
    "df = df.head(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-volleyball",
   "metadata": {
    "title": "get audio data and save to csv"
   },
   "outputs": [],
   "source": [
    "\n",
    "# run this bit only once\n",
    "from multiprocessing import Pool\n",
    "\n",
    "num_classes = len(list(df[\"class\"].unique()))\n",
    "\n",
    "\n",
    "def parallelize_dataframe(df, func, n_cores=8):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_mfcc(df):\n",
    "    df[\"audio\"] = df[\"full_file_name\"].apply(extract_mfcc)\n",
    "    return df\n",
    "\n",
    "\n",
    "# df[\"full_file_name\"] = INPUT_PATH + \"/audio/\"+df[\"slice_file_name\"]\n",
    "\n",
    "# df = parallelize_dataframe(df, apply_mfcc)\n",
    "# df.to_pickle(INPUT_PATH + \"/metadata/processed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-burton",
   "metadata": {
    "title": "read from saved data"
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(INPUT_PATH + \"/metadata/processed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_images, valid_images = train_test_split(\n",
    "    df, test_size=0.33, stratify=df[\"classID\"]\n",
    ")\n",
    "\n",
    "train_image_paths, valid_image_paths = (\n",
    "    train_images[\"audio\"].values,\n",
    "    valid_images[\"audio\"].values,\n",
    ")\n",
    "\n",
    "train_targets, valid_targets = (\n",
    "    train_images[\"classID\"].values,\n",
    "    valid_images[\"classID\"].values,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-cycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\"image\": self.x, \"targets\": self.y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-wedding",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "train_dataset = AudioDataset(\n",
    "    torch.stack([x for x in train_image_paths]),\n",
    "    torch.stack([torch.tensor(x) for x in train_targets]),\n",
    ")\n",
    "valid_dataset = AudioDataset(\n",
    "    torch.stack([x for x in valid_image_paths]),\n",
    "    torch.stack([torch.tensor(x) for x in valid_targets]),\n",
    ")\n",
    "\n",
    "print(len(train_dataset), len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-destruction",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#  Callbacks\n",
    "model = Model(10)\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor=\"valid_loss\",\n",
    "    model_path=os.path.join(MODEL_PATH, MODEL_NAME + \".bin\"),\n",
    "    patience=3,\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "tb = TensorBoardLogger()\n",
    "grc = GradientClipping(5)\n",
    "pl = PlotLoss(30)\n",
    "\n",
    "count_parameters(model, showtable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-consent",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    valid_dataset=valid_dataset,\n",
    "    train_bs=TRAIN_BATCH_SIZE,\n",
    "    valid_bs=VALID_BATCH_SIZE,\n",
    "    device=\"cuda\",\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[grc, pl, tb],\n",
    "    fp16=True,\n",
    ")\n",
    "# -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-newspaper",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-tampa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
